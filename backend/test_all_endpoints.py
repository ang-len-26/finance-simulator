#!/usr/bin/env python3
"""
ðŸ§ª TEST_ALL_ENDPOINTS.PY - FinTrack Sistema Completo de Testing
===============================================================

Pruebas automÃ¡ticas para todos los endpoints del sistema FinTrack
- CORE: AutenticaciÃ³n y setup
- ACCOUNTS: Cuentas y filtros  
- TRANSACTIONS: Transacciones, categorÃ­as y alertas
- ANALYTICS: Reportes y anÃ¡lisis
- GOALS: Metas financieras y contribuciones

Uso: python test_all_endpoints.py
"""

import requests
import json
import sys
import os
from datetime import datetime, timedelta
from decimal import Decimal
import time

# =============================================================================
# ðŸ”§ CONFIGURACIÃ“N
# =============================================================================

# URL base del API (cambiar segÃºn entorno)
BASE_URL = "http://localhost:8000/api"
# BASE_URL = "https://your-deployed-api.com/api"

# ConfiguraciÃ³n de testing
TEST_CONFIG = {
    'timeout': 30,
    'max_retries': 3,
    'demo_user_credentials': None,
    'tokens': None,
    'created_resources': {
        'accounts': [],
        'transactions': [],
        'goals': [],
        'categories': []
    }
}

# Colores para output
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'  
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    END = '\033[0m'

# =============================================================================
# ðŸ› ï¸ UTILIDADES
# =============================================================================

def print_section(title):
    """Imprime secciÃ³n con formato"""
    print(f"\n{Colors.CYAN}{'='*60}{Colors.END}")
    print(f"{Colors.CYAN}{Colors.BOLD}ðŸ§ª {title}{Colors.END}")
    print(f"{Colors.CYAN}{'='*60}{Colors.END}")

def print_test(endpoint, method="GET", description=""):
    """Imprime test en progreso"""
    print(f"{Colors.YELLOW}ðŸ”„ [{method}] {endpoint}{Colors.END}")
    if description:
        print(f"   {description}")

def print_success(message):
    """Imprime Ã©xito"""
    print(f"{Colors.GREEN}âœ… {message}{Colors.END}")

def print_error(message):
    """Imprime error"""
    print(f"{Colors.RED}âŒ {message}{Colors.END}")

def print_warning(message):
    """Imprime advertencia"""
    print(f"{Colors.YELLOW}âš ï¸ {message}{Colors.END}")

def print_info(message):
    """Imprime informaciÃ³n"""
    print(f"{Colors.BLUE}â„¹ï¸ {message}{Colors.END}")

def make_request(method, endpoint, data=None, params=None, headers=None, auth_required=True):
    """Realiza request con manejo de errores y retry"""
    url = f"{BASE_URL}{endpoint}"
    
    # Headers por defecto
    default_headers = {'Content-Type': 'application/json'}
    
    # Agregar autenticaciÃ³n si es requerida
    if auth_required and TEST_CONFIG.get('tokens', {}).get('access'):
        default_headers['Authorization'] = f"Bearer {TEST_CONFIG['tokens']['access']}"
    
    if headers:
        default_headers.update(headers)
    
    # Realizar request con reintentos
    for attempt in range(TEST_CONFIG['max_retries']):
        try:
            response = requests.request(
                method=method,
                url=url,
                json=data,
                params=params,
                headers=default_headers,
                timeout=TEST_CONFIG['timeout']
            )
            return response
            
        except requests.exceptions.RequestException as e:
            if attempt == TEST_CONFIG['max_retries'] - 1:
                print_error(f"Error de conexiÃ³n despuÃ©s de {TEST_CONFIG['max_retries']} intentos: {str(e)}")
                return None
            time.sleep(1)  # Esperar antes del retry
    
    return None

def validate_response(response, expected_status=200, endpoint=""):
    """Valida respuesta del servidor"""
    if response is None:
        print_error(f"No se pudo conectar al endpoint: {endpoint}")
        return False
    
    if response.status_code != expected_status:
        print_error(f"Status {response.status_code} (esperado {expected_status})")
        try:
            error_data = response.json()
            print_error(f"Error: {error_data}")
        except:
            print_error(f"Respuesta: {response.text[:200]}")
        return False
    
    try:
        data = response.json()
        print_success(f"Status {response.status_code} - Respuesta vÃ¡lida")
        return data
    except json.JSONDecodeError:
        print_error("Respuesta no es JSON vÃ¡lido")
        return False

# =============================================================================
# ðŸ” MÃ“DULO CORE - AUTENTICACIÃ“N Y SETUP
# =============================================================================

def test_core_module():
    """Prueba todos los endpoints del mÃ³dulo CORE"""
    print_section("MÃ“DULO CORE - AUTENTICACIÃ“N Y SETUP")
    
    results = {
        'total': 0,
        'passed': 0,
        'failed': 0,
        'details': []
    }
    
    # 1. Test Setup - Migraciones
    print_test("/setup/run-migrations/", "POST", "Ejecutar migraciones")
    results['total'] += 1
    
    response = make_request("POST", "/setup/run-migrations/", auth_required=False)
    data = validate_response(response, 200, "/setup/run-migrations/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Migraciones ejecutadas correctamente")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error ejecutando migraciones")
    
    # 2. Test Setup - Crear Superusuario
    print_test("/setup/create-superuser/", "POST", "Crear superusuario")
    results['total'] += 1
    
    response = make_request("POST", "/setup/create-superuser/", auth_required=False)
    data = validate_response(response, 201, "/setup/create-superuser/")
    
    if data and data.get('status') == 'success':
        results['passed'] += 1
        results['details'].append("âœ… Superusuario creado correctamente")
    else:
        # Puede fallar si ya existe - verificar mensaje
        if data and 'already exists' in data.get('message', ''):
            results['passed'] += 1
            results['details'].append("âœ… Superusuario ya existe")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error creando superusuario")
    
    # 3. Test Crear Usuario Demo
    print_test("/auth/demo/", "POST", "Crear usuario demo con datos")
    results['total'] += 1
    
    response = make_request("POST", "/auth/demo/", auth_required=False)
    data = validate_response(response, 200, "/auth/demo/")
    
    if data and data.get('access') and data.get('refresh'):
        # Guardar tokens para siguientes pruebas
        TEST_CONFIG['tokens'] = {
            'access': data['access'],
            'refresh': data['refresh']
        }
        results['passed'] += 1
        results['details'].append("âœ… Usuario demo creado con tokens JWT")
        print_info(f"Token obtenido: {data['access'][:50]}...")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error creando usuario demo")
        print_error("No se pudieron obtener tokens - las pruebas autenticadas fallarÃ¡n")
    
    # 4. Test JWT Token Refresh
    if TEST_CONFIG.get('tokens', {}).get('refresh'):
        print_test("/token/refresh/", "POST", "Refrescar token JWT")
        results['total'] += 1
        
        refresh_data = {"refresh": TEST_CONFIG['tokens']['refresh']}
        response = make_request("POST", "/token/refresh/", data=refresh_data, auth_required=False)
        data = validate_response(response, 200, "/token/refresh/")
        
        if data and data.get('access'):
            # Actualizar token
            TEST_CONFIG['tokens']['access'] = data['access']
            results['passed'] += 1
            results['details'].append("âœ… Token JWT refrescado exitosamente")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error refrescando token JWT")
    
    # 5. Test Perfil de Usuario
    if TEST_CONFIG.get('tokens', {}).get('access'):
        print_test("/auth/profile/", "GET", "Obtener perfil del usuario autenticado")
        results['total'] += 1
        
        response = make_request("GET", "/auth/profile/")
        data = validate_response(response, 200, "/auth/profile/")
        
        if data and 'username' in data:
            results['passed'] += 1
            results['details'].append(f"âœ… Perfil obtenido - Usuario: {data.get('username')}")
            print_info(f"Usuario demo: {data.get('username')}, Es demo: {data.get('is_demo')}")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error obteniendo perfil de usuario")
    
    # 6. Test Registro de Usuario Normal (opcional)
    print_test("/auth/register/", "POST", "Registro de usuario normal")
    results['total'] += 1
    
    register_data = {
        "username": f"test_user_{int(time.time())}",
        "email": f"test_{int(time.time())}@fintrack.com",
        "password": "testpass123",
        "password_confirm": "testpass123"
    }
    
    response = make_request("POST", "/auth/register/", data=register_data, auth_required=False)
    data = validate_response(response, 201, "/auth/register/")
    
    if data and data.get('user_id'):
        results['passed'] += 1
        results['details'].append("âœ… Usuario normal registrado exitosamente")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error registrando usuario normal")
    
    return results

# =============================================================================
# ðŸ¦ MÃ“DULO ACCOUNTS - CUENTAS Y FILTROS
# =============================================================================

def test_accounts_module():
    """Prueba todos los endpoints del mÃ³dulo ACCOUNTS"""
    print_section("MÃ“DULO ACCOUNTS - CUENTAS Y FILTROS")
    
    if not TEST_CONFIG.get('tokens', {}).get('access'):
        print_error("No hay token de autenticaciÃ³n - saltando pruebas de ACCOUNTS")
        return {'total': 0, 'passed': 0, 'failed': 0, 'details': ['âŒ Sin autenticaciÃ³n']}
    
    results = {
        'total': 0,
        'passed': 0,
        'failed': 0,
        'details': []
    }
    
    # 1. Test Listar Cuentas
    print_test("/accounts/", "GET", "Listar todas las cuentas del usuario")
    results['total'] += 1
    
    response = make_request("GET", "/accounts/")
    data = validate_response(response, 200, "/accounts/")
    
    if data:
        accounts_count = len(data.get('results', data))
        results['passed'] += 1
        results['details'].append(f"âœ… Cuentas listadas: {accounts_count} encontradas")
        print_info(f"Cuentas demo disponibles: {accounts_count}")
        
        # Guardar IDs de cuentas para otros tests
        if isinstance(data, dict) and 'results' in data:
            TEST_CONFIG['created_resources']['accounts'] = [acc['id'] for acc in data['results']]
        elif isinstance(data, list):
            TEST_CONFIG['created_resources']['accounts'] = [acc['id'] for acc in data]
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error listando cuentas")
    
    # 2. Test Crear Nueva Cuenta
    print_test("/accounts/", "POST", "Crear nueva cuenta de ahorro")
    results['total'] += 1
    
    new_account_data = {
        "name": "Cuenta Test",
        "bank_name": "Banco Test",
        "account_type": "savings",
        "initial_balance": "1000.00",
        "currency": "PEN"
    }
    
    response = make_request("POST", "/accounts/", data=new_account_data)
    data = validate_response(response, 201, "/accounts/")
    
    if data and data.get('id'):
        new_account_id = data['id']
        TEST_CONFIG['created_resources']['accounts'].append(new_account_id)
        results['passed'] += 1
        results['details'].append(f"âœ… Cuenta creada - ID: {new_account_id}")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error creando cuenta")
    
    # 3. Test Obtener Cuenta EspecÃ­fica
    if TEST_CONFIG['created_resources']['accounts']:
        account_id = TEST_CONFIG['created_resources']['accounts'][0]
        print_test(f"/accounts/{account_id}/", "GET", "Obtener detalles de cuenta especÃ­fica")
        results['total'] += 1
        
        response = make_request("GET", f"/accounts/{account_id}/")
        data = validate_response(response, 200, f"/accounts/{account_id}/")
        
        if data and data.get('id') == account_id:
            results['passed'] += 1
            results['details'].append(f"âœ… Cuenta especÃ­fica obtenida - {data.get('name')}")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error obteniendo cuenta especÃ­fica")
    
    # 4. Test Resumen Financiero Global
    print_test("/accounts/summary/", "GET", "Resumen financiero global")
    results['total'] += 1
    
    response = make_request("GET", "/accounts/summary/")
    data = validate_response(response, 200, "/accounts/summary/")
    
    if data and 'total_balance' in data:
        results['passed'] += 1
        results['details'].append(f"âœ… Resumen obtenido - Balance total: ${data.get('total_balance', 0)}")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo resumen financiero")
    
    # 5. Test Filtros - Cuentas Activas
    print_test("/accounts/?is_active=true", "GET", "Filtrar solo cuentas activas")
    results['total'] += 1
    
    response = make_request("GET", "/accounts/", params={"is_active": "true"})
    data = validate_response(response, 200, "/accounts/ con filtro is_active")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtro is_active funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtro is_active")
    
    # 6. Test Filtros - Por Tipo de Cuenta
    print_test("/accounts/?account_type=checking", "GET", "Filtrar cuentas corrientes")
    results['total'] += 1
    
    response = make_request("GET", "/accounts/", params={"account_type": "checking"})
    data = validate_response(response, 200, "/accounts/ con filtro account_type")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtro account_type funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtro account_type")
    
    # 7. Test Filtros - Rango de Balance
    print_test("/accounts/?min_balance=1000&max_balance=10000", "GET", "Filtrar por rango de balance")
    results['total'] += 1
    
    response = make_request("GET", "/accounts/", params={"min_balance": "1000", "max_balance": "10000"})
    data = validate_response(response, 200, "/accounts/ con filtros de balance")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtros de balance funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtros de balance")
    
    # 8. Test Historial de Balance
    if TEST_CONFIG['created_resources']['accounts']:
        account_id = TEST_CONFIG['created_resources']['accounts'][0]
        print_test(f"/accounts/{account_id}/balance_history/", "GET", "Historial de balance (30 dÃ­as)")
        results['total'] += 1
        
        response = make_request("GET", f"/accounts/{account_id}/balance_history/")
        data = validate_response(response, 200, f"/accounts/{account_id}/balance_history/")
        
        if data:
            results['passed'] += 1
            results['details'].append("âœ… Historial de balance obtenido")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error obteniendo historial de balance")
    
    # 9. Test Transacciones de Cuenta
    if TEST_CONFIG['created_resources']['accounts']:
        account_id = TEST_CONFIG['created_resources']['accounts'][0]
        print_test(f"/accounts/{account_id}/transactions/", "GET", "Transacciones de cuenta especÃ­fica")
        results['total'] += 1
        
        response = make_request("GET", f"/accounts/{account_id}/transactions/")
        data = validate_response(response, 200, f"/accounts/{account_id}/transactions/")
        
        if data:
            transactions_count = len(data.get('results', data))
            results['passed'] += 1
            results['details'].append(f"âœ… Transacciones de cuenta: {transactions_count} encontradas")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error obteniendo transacciones de cuenta")
    
    return results

# =============================================================================
# ðŸ’³ MÃ“DULO TRANSACTIONS - TRANSACCIONES, CATEGORÃAS Y ALERTAS
# =============================================================================

def test_transactions_module():
    """Prueba todos los endpoints del mÃ³dulo TRANSACTIONS"""
    print_section("MÃ“DULO TRANSACTIONS - TRANSACCIONES, CATEGORÃAS Y ALERTAS")
    
    if not TEST_CONFIG.get('tokens', {}).get('access'):
        print_error("No hay token de autenticaciÃ³n - saltando pruebas de TRANSACTIONS")
        return {'total': 0, 'passed': 0, 'failed': 0, 'details': ['âŒ Sin autenticaciÃ³n']}
    
    results = {
        'total': 0,
        'passed': 0,
        'failed': 0,
        'details': []
    }
    
    # === CATEGORÃAS FIRST (necesarias para transacciones) ===
    
    # 1. Test Crear CategorÃ­as por Defecto
    print_test("/categories/create_defaults/", "POST", "Crear categorÃ­as predeterminadas")
    results['total'] += 1
    
    response = make_request("POST", "/categories/create_defaults/")
    data = validate_response(response, 201, "/categories/create_defaults/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… CategorÃ­as por defecto creadas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error creando categorÃ­as por defecto")
    
    # 2. Test Listar CategorÃ­as
    print_test("/categories/", "GET", "Listar todas las categorÃ­as")
    results['total'] += 1
    
    response = make_request("GET", "/categories/")
    data = validate_response(response, 200, "/categories/")
    
    if data:
        categories_count = len(data.get('results', data))
        results['passed'] += 1
        results['details'].append(f"âœ… CategorÃ­as listadas: {categories_count} encontradas")
        
        # Guardar IDs de categorÃ­as
        if isinstance(data, dict) and 'results' in data:
            TEST_CONFIG['created_resources']['categories'] = [cat['id'] for cat in data['results'][:3]]
        elif isinstance(data, list):
            TEST_CONFIG['created_resources']['categories'] = [cat['id'] for cat in data[:3]]
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error listando categorÃ­as")
    
    # 3. Test CategorÃ­as por Tipo
    print_test("/categories/by_type/", "GET", "CategorÃ­as agrupadas por tipo")
    results['total'] += 1
    
    response = make_request("GET", "/categories/by_type/")
    data = validate_response(response, 200, "/categories/by_type/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… CategorÃ­as agrupadas por tipo obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo categorÃ­as por tipo")
    
    # === TRANSACCIONES ===
    
    # 4. Test Listar Transacciones
    print_test("/transactions/", "GET", "Listar todas las transacciones")
    results['total'] += 1
    
    response = make_request("GET", "/transactions/")
    data = validate_response(response, 200, "/transactions/")
    
    if data:
        transactions_count = len(data.get('results', data))
        results['passed'] += 1
        results['details'].append(f"âœ… Transacciones listadas: {transactions_count} encontradas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error listando transacciones")
    
    # 5. Test Crear Nueva TransacciÃ³n
    if TEST_CONFIG['created_resources']['accounts'] and TEST_CONFIG['created_resources']['categories']:
        print_test("/transactions/", "POST", "Crear nueva transacciÃ³n de gasto")
        results['total'] += 1
        
        new_transaction_data = {
            "title": "Compra Test",
            "description": "TransacciÃ³n de prueba automatizada",
            "amount": "50.00",
            "type": "expense",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "from_account": TEST_CONFIG['created_resources']['accounts'][0],
            "category": TEST_CONFIG['created_resources']['categories'][0]
        }
        
        response = make_request("POST", "/transactions/", data=new_transaction_data)
        data = validate_response(response, 201, "/transactions/")
        
        if data and data.get('id'):
            new_transaction_id = data['id']
            TEST_CONFIG['created_resources']['transactions'].append(new_transaction_id)
            results['passed'] += 1
            results['details'].append(f"âœ… TransacciÃ³n creada - ID: {new_transaction_id}")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error creando transacciÃ³n")
    
    # 6. Test Transacciones Recientes
    print_test("/transactions/recent/", "GET", "Ãšltimas 10 transacciones")
    results['total'] += 1
    
    response = make_request("GET", "/transactions/recent/")
    data = validate_response(response, 200, "/transactions/recent/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Transacciones recientes obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo transacciones recientes")
    
    # 7. Test Dashboard de Transacciones
    print_test("/transactions/dashboard/", "GET", "Dashboard con mÃ©tricas del perÃ­odo")
    results['total'] += 1
    
    response = make_request("GET", "/transactions/dashboard/")
    data = validate_response(response, 200, "/transactions/dashboard/")
    
    if data and 'metrics' in data:
        results['passed'] += 1
        results['details'].append("âœ… Dashboard de transacciones obtenido")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo dashboard de transacciones")
    
    # 8. Test Transacciones por Tipo
    print_test("/transactions/by_type/", "GET", "Transacciones agrupadas por tipo")
    results['total'] += 1
    
    response = make_request("GET", "/transactions/by_type/")
    data = validate_response(response, 200, "/transactions/by_type/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Transacciones agrupadas por tipo obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo transacciones por tipo")
    
    # 9. Test BÃºsqueda de Transacciones
    print_test("/transactions/search/?q=test", "GET", "BÃºsqueda avanzada de transacciones")
    results['total'] += 1
    
    response = make_request("GET", "/transactions/search/", params={"q": "test"})
    data = validate_response(response, 200, "/transactions/search/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… BÃºsqueda de transacciones funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error en bÃºsqueda de transacciones")
    
    # 10. Test Filtros de Transacciones - Por Tipo
    print_test("/transactions/?type=expense", "GET", "Filtrar solo gastos")
    results['total'] += 1
    
    response = make_request("GET", "/transactions/", params={"type": "expense"})
    data = validate_response(response, 200, "/transactions/ con filtro type")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtro por tipo funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtro por tipo")
    
    # 11. Test Filtros - Rango de Fechas
    print_test("/transactions/?date_after=2024-01-01", "GET", "Filtrar por fecha")
    results['total'] += 1
    
    response = make_request("GET", "/transactions/", params={
        "date_after": "2024-01-01",
        "date_before": "2024-12-31"
    })
    data = validate_response(response, 200, "/transactions/ con filtros de fecha")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtros de fecha funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtros de fecha")
    
    # 12. Test Alertas de Presupuesto
    print_test("/budget-alerts/", "GET", "Listar alertas de presupuesto")
    results['total'] += 1
    
    response = make_request("GET", "/budget-alerts/")  
    data = validate_response(response, 200, "/budget-alerts/")
    
    if data:
        alerts_count = len(data.get('results', data))
        results['passed'] += 1
        results['details'].append(f"âœ… Alertas listadas: {alerts_count} encontradas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error listando alertas de presupuesto")
    
    # 13. Test Alertas No LeÃ­das
    print_test("/budget-alerts/unread/", "GET", "Alertas no leÃ­das")
    results['total'] += 1
    
    response = make_request("GET", "/budget-alerts/unread/")
    data = validate_response(response, 200, "/budget-alerts/unread/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Alertas no leÃ­das obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo alertas no leÃ­das")
    
    return results

# =============================================================================
# ðŸ“Š MÃ“DULO ANALYTICS - REPORTES Y ANÃLISIS
# =============================================================================

def test_analytics_module():
    """Prueba todos los endpoints del mÃ³dulo ANALYTICS"""
    print_section("MÃ“DULO ANALYTICS - REPORTES Y ANÃLISIS")
    
    if not TEST_CONFIG.get('tokens', {}).get('access'):
        print_error("No hay token de autenticaciÃ³n - saltando pruebas de ANALYTICS")
        return {'total': 0, 'passed': 0, 'failed': 0, 'details': ['âŒ Sin autenticaciÃ³n']}
    
    results = {
        'total': 0,
        'passed': 0,
        'failed': 0,
        'details': []
    }
    
    # 1. Test MÃ©tricas Principales
    print_test("/reports/metrics/", "GET", "MÃ©tricas principales con comparativas")
    results['total'] += 1
    
    response = make_request("GET", "/reports/metrics/")
    data = validate_response(response, 200, "/reports/metrics/")
    
    if data and 'metrics' in data:
        results['passed'] += 1
        results['details'].append("âœ… MÃ©tricas principales obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo mÃ©tricas principales")
    
    # 2. Test Ingresos vs Gastos
    print_test("/reports/income-vs-expenses/", "GET", "GrÃ¡fico ingresos vs gastos mensual")
    results['total'] += 1
    
    response = make_request("GET", "/reports/income-vs-expenses/")
    data = validate_response(response, 200, "/reports/income-vs-expenses/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Reporte ingresos vs gastos obtenido")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo ingresos vs gastos")
    
    # 3. Test Timeline de Balance
    print_test("/reports/balance-timeline/", "GET", "Timeline balance acumulado")
    results['total'] += 1
    
    response = make_request("GET", "/reports/balance-timeline/")
    data = validate_response(response, 200, "/reports/balance-timeline/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Timeline de balance obtenido")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo timeline de balance")
    
    # 4. Test DistribuciÃ³n por CategorÃ­as
    print_test("/reports/category-distribution/", "GET", "DistribuciÃ³n por categorÃ­as (pie)")
    results['total'] += 1
    
    response = make_request("GET", "/reports/category-distribution/")
    data = validate_response(response, 200, "/reports/category-distribution/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… DistribuciÃ³n por categorÃ­as obtenida")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo distribuciÃ³n por categorÃ­as")
    
    # 5. Test Top CategorÃ­as
    print_test("/reports/top-categories/", "GET", "Top 5 categorÃ­as con tendencias")
    results['total'] += 1
    
    response = make_request("GET", "/reports/top-categories/")
    data = validate_response(response, 200, "/reports/top-categories/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Top categorÃ­as obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo top categorÃ­as")
    
    # 6. Test Transacciones Recientes (Analytics)
    print_test("/reports/recent-transactions/", "GET", "Transacciones recientes con Ã­conos")
    results['total'] += 1
    
    response = make_request("GET", "/reports/recent-transactions/")
    data = validate_response(response, 200, "/reports/recent-transactions/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Transacciones recientes (analytics) obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo transacciones recientes (analytics)")
    
    # 7. Test MÃ©tricas Financieras
    print_test("/reports/financial-metrics/", "GET", "MÃ©tricas precalculadas por perÃ­odo")
    results['total'] += 1
    
    response = make_request("GET", "/reports/financial-metrics/")
    data = validate_response(response, 200, "/reports/financial-metrics/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… MÃ©tricas financieras obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo mÃ©tricas financieras")
    
    # 8. Test Alertas de Reportes
    print_test("/reports/alerts/", "GET", "Obtener alertas de presupuesto")
    results['total'] += 1
    
    response = make_request("GET", "/reports/alerts/")
    data = validate_response(response, 200, "/reports/alerts/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Alertas de reportes obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo alertas de reportes")
    
    # 9. Test Tendencias de CategorÃ­as
    print_test("/reports/category-trends/", "GET", "Tendencias de categorÃ­as en tiempo")
    results['total'] += 1
    
    response = make_request("GET", "/reports/category-trends/")
    data = validate_response(response, 200, "/reports/category-trends/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Tendencias de categorÃ­as obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo tendencias de categorÃ­as")
    
    # 10. Test Dashboard Completo
    print_test("/reports-overview/", "GET", "Dashboard completo (1 llamada)")
    results['total'] += 1
    
    response = make_request("GET", "/reports-overview/")
    data = validate_response(response, 200, "/reports-overview/")
    
    if data and 'metrics' in data and 'charts' in data:
        results['passed'] += 1
        results['details'].append("âœ… Dashboard completo obtenido")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo dashboard completo")
    
    # 11. Test Ratios Financieros
    print_test("/financial-ratios/", "GET", "Ratios financieros profesionales")
    results['total'] += 1
    
    response = make_request("GET", "/financial-ratios/")
    data = validate_response(response, 200, "/financial-ratios/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Ratios financieros obtenidos")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo ratios financieros")
    
    # 12. Test Filtros - PerÃ­odo Mensual
    print_test("/reports/metrics/?period=monthly", "GET", "MÃ©tricas con filtro perÃ­odo mensual")
    results['total'] += 1
    
    response = make_request("GET", "/reports/metrics/", params={"period": "monthly"})
    data = validate_response(response, 200, "/reports/metrics/ con filtro perÃ­odo")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtro de perÃ­odo funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtro de perÃ­odo")
    
    # 13. Test Filtros - Fechas Personalizadas
    print_test("/reports/metrics/?start_date=2024-01-01&end_date=2024-12-31", "GET", "MÃ©tricas con rango personalizado")
    results['total'] += 1
    
    response = make_request("GET", "/reports/metrics/", params={
        "start_date": "2024-01-01",
        "end_date": "2024-12-31"
    })
    data = validate_response(response, 200, "/reports/metrics/ con fechas personalizadas")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtros de fechas personalizadas funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtros de fechas personalizadas")
    
    return results

# =============================================================================
# ðŸŽ¯ MÃ“DULO GOALS - METAS FINANCIERAS Y CONTRIBUCIONES
# =============================================================================

def test_goals_module():
    """Prueba todos los endpoints del mÃ³dulo GOALS"""
    print_section("MÃ“DULO GOALS - METAS FINANCIERAS Y CONTRIBUCIONES")
    
    if not TEST_CONFIG.get('tokens', {}).get('access'):
        print_error("No hay token de autenticaciÃ³n - saltando pruebas de GOALS")
        return {'total': 0, 'passed': 0, 'failed': 0, 'details': ['âŒ Sin autenticaciÃ³n']}
    
    results = {
        'total': 0,
        'passed': 0,
        'failed': 0,
        'details': []
    }
    
    # 1. Test Crear Plantillas de Metas por Defecto
    print_test("/setup/create-goal-templates/", "POST", "Crear plantillas por defecto")
    results['total'] += 1
    
    response = make_request("POST", "/setup/create-goal-templates/")
    data = validate_response(response, 201, "/setup/create-goal-templates/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Plantillas de metas creadas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error creando plantillas de metas")
    
    # 2. Test Listar Plantillas
    print_test("/goal-templates/", "GET", "Listar plantillas disponibles")
    results['total'] += 1
    
    response = make_request("GET", "/goal-templates/")
    data = validate_response(response, 200, "/goal-templates/")
    
    if data:
        templates_count = len(data.get('results', data))
        results['passed'] += 1
        results['details'].append(f"âœ… Plantillas listadas: {templates_count} encontradas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error listando plantillas")
    
    # 3. Test Plantillas por CategorÃ­a
    print_test("/goal-templates/by_category/", "GET", "Plantillas agrupadas por tipo")
    results['total'] += 1
    
    response = make_request("GET", "/goal-templates/by_category/")
    data = validate_response(response, 200, "/goal-templates/by_category/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Plantillas agrupadas por categorÃ­a obtenidas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo plantillas por categorÃ­a")
    
    # 4. Test Crear Meta Financiera
    if TEST_CONFIG['created_resources']['accounts']:
        print_test("/goals/", "POST", "Crear nueva meta financiera")
        results['total'] += 1
        
        new_goal_data = {
            "title": "Meta Test Automatizada",
            "description": "Meta creada por el sistema de testing",
            "goal_type": "savings",
            "target_amount": "5000.00",
            "target_date": (datetime.now() + timedelta(days=365)).strftime("%Y-%m-%d"),
            "priority": "medium",
            "associated_account": TEST_CONFIG['created_resources']['accounts'][0]
        }
        
        response = make_request("POST", "/goals/", data=new_goal_data)
        data = validate_response(response, 201, "/goals/")
        
        if data and data.get('id'):
            new_goal_id = data['id']
            TEST_CONFIG['created_resources']['goals'].append(new_goal_id)
            results['passed'] += 1
            results['details'].append(f"âœ… Meta creada - ID: {new_goal_id}")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error creando meta")
    
    # 5. Test Listar Metas
    print_test("/goals/", "GET", "Listar todas las metas")
    results['total'] += 1
    
    response = make_request("GET", "/goals/")
    data = validate_response(response, 200, "/goals/")
    
    if data:
        goals_count = len(data.get('results', data))
        results['passed'] += 1
        results['details'].append(f"âœ… Metas listadas: {goals_count} encontradas")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error listando metas")
    
    # 6. Test Dashboard de Metas
    print_test("/goals/dashboard/", "GET", "Dashboard completo de metas")
    results['total'] += 1
    
    response = make_request("GET", "/goals/dashboard/")
    data = validate_response(response, 200, "/goals/dashboard/")
    
    if data and 'summary' in data:
        results['passed'] += 1
        results['details'].append("âœ… Dashboard de metas obtenido")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo dashboard de metas")
    
    # 7. Test Resumen de Metas
    print_test("/goals/summary/", "GET", "Resumen rÃ¡pido para widgets")
    results['total'] += 1
    
    response = make_request("GET", "/goals/summary/")
    data = validate_response(response, 200, "/goals/summary/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Resumen de metas obtenido")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo resumen de metas")
    
    # 8. Test Agregar ContribuciÃ³n a Meta
    if TEST_CONFIG['created_resources']['goals'] and TEST_CONFIG['created_resources']['accounts']:
        goal_id = TEST_CONFIG['created_resources']['goals'][0]
        print_test(f"/goals/{goal_id}/add_contribution/", "POST", "Agregar contribuciÃ³n a meta")
        results['total'] += 1
        
        contribution_data = {
            "amount": "100.00",
            "from_account": TEST_CONFIG['created_resources']['accounts'][0],
            "notes": "ContribuciÃ³n de prueba automatizada"
        }
        
        response = make_request("POST", f"/goals/{goal_id}/add_contribution/", data=contribution_data)
        data = validate_response(response, 201, f"/goals/{goal_id}/add_contribution/")
        
        if data:
            results['passed'] += 1
            results['details'].append("âœ… ContribuciÃ³n agregada exitosamente")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error agregando contribuciÃ³n")
    
    # 9. Test Ver Contribuciones de Meta
    if TEST_CONFIG['created_resources']['goals']:
        goal_id = TEST_CONFIG['created_resources']['goals'][0]
        print_test(f"/goals/{goal_id}/contributions/", "GET", "Ver contribuciones de meta")
        results['total'] += 1
        
        response = make_request("GET", f"/goals/{goal_id}/contributions/")
        data = validate_response(response, 200, f"/goals/{goal_id}/contributions/")
        
        if data:
            contributions_count = len(data.get('results', data))
            results['passed'] += 1
            results['details'].append(f"âœ… Contribuciones listadas: {contributions_count} encontradas")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error listando contribuciones")
    
    # 10. Test AnÃ¡lisis de Meta
    if TEST_CONFIG['created_resources']['goals']:
        goal_id = TEST_CONFIG['created_resources']['goals'][0]
        print_test(f"/goals/{goal_id}/analytics/", "GET", "AnÃ¡lisis detallado de meta")
        results['total'] += 1
        
        response = make_request("GET", f"/goals/{goal_id}/analytics/")
        data = validate_response(response, 200, f"/goals/{goal_id}/analytics/")
        
        if data:
            results['passed'] += 1
            results['details'].append("âœ… AnÃ¡lisis de meta obtenido")
        else:
            results['failed'] += 1
            results['details'].append("âŒ Error obteniendo anÃ¡lisis de meta")
    
    # 11. Test Calendario de Metas
    print_test("/goals-calendar/", "GET", "Vista calendario de metas")
    results['total'] += 1
    
    current_date = datetime.now()
    response = make_request("GET", "/goals-calendar/", params={
        "year": current_date.year,
        "month": current_date.month
    })
    data = validate_response(response, 200, "/goals-calendar/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Calendario de metas obtenido")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo calendario de metas")
    
    # 12. Test Insights Inteligentes
    print_test("/goals-insights/", "GET", "Insights inteligentes")
    results['total'] += 1
    
    response = make_request("GET", "/goals-insights/")
    data = validate_response(response, 200, "/goals-insights/")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Insights inteligentes obtenidos")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error obteniendo insights inteligentes")
    
    # 13. Test Filtros - Metas Activas
    print_test("/goals/?status=active", "GET", "Filtrar solo metas activas")
    results['total'] += 1
    
    response = make_request("GET", "/goals/", params={"status": "active"})
    data = validate_response(response, 200, "/goals/ con filtro status")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtro por status funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtro por status")
    
    # 14. Test Filtros - Por Tipo de Meta
    print_test("/goals/?goal_type=savings", "GET", "Filtrar metas de ahorro")
    results['total'] += 1
    
    response = make_request("GET", "/goals/", params={"goal_type": "savings"})
    data = validate_response(response, 200, "/goals/ con filtro goal_type")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtro por tipo de meta funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtro por tipo de meta")
    
    # 15. Test Filtros - Rango de Progreso
    print_test("/goals/?min_progress=25&max_progress=75", "GET", "Filtrar por rango de progreso")
    results['total'] += 1
    
    response = make_request("GET", "/goals/", params={"min_progress": "25", "max_progress": "75"})
    data = validate_response(response, 200, "/goals/ con filtros de progreso")
    
    if data:
        results['passed'] += 1
        results['details'].append("âœ… Filtros de progreso funcionando")
    else:
        results['failed'] += 1
        results['details'].append("âŒ Error con filtros de progreso")
    
    return results

# =============================================================================
# ðŸ“Š FUNCIÃ“N PRINCIPAL DE TESTING
# =============================================================================

def run_all_tests():
    """Ejecuta todas las pruebas y genera reporte final"""
    print(f"{Colors.BOLD}{Colors.BLUE}")
    print("ðŸ§ª INICIANDO TESTING COMPLETO DE FINTRACK")
    print("="*80)
    print(f"Base URL: {BASE_URL}")
    print(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{Colors.END}")
    
    # Resultados consolidados
    total_results = {
        'modules': {},
        'total_tests': 0,
        'total_passed': 0,
        'total_failed': 0,
        'start_time': datetime.now(),
        'end_time': None
    }
    
    # === EJECUTAR TESTS POR MÃ“DULO ===
    
    # 1. CORE
    try:
        core_results = test_core_module()
        total_results['modules']['CORE'] = core_results
        total_results['total_tests'] += core_results['total']
        total_results['total_passed'] += core_results['passed']
        total_results['total_failed'] += core_results['failed']
    except Exception as e:
        print_error(f"Error crÃ­tico en mÃ³dulo CORE: {str(e)}")
        total_results['modules']['CORE'] = {'total': 0, 'passed': 0, 'failed': 1, 'details': [f"âŒ Error crÃ­tico: {str(e)}"]}
        total_results['total_failed'] += 1
    
    # 2. ACCOUNTS
    try:
        accounts_results = test_accounts_module()
        total_results['modules']['ACCOUNTS'] = accounts_results
        total_results['total_tests'] += accounts_results['total']
        total_results['total_passed'] += accounts_results['passed']
        total_results['total_failed'] += accounts_results['failed']
    except Exception as e:
        print_error(f"Error crÃ­tico en mÃ³dulo ACCOUNTS: {str(e)}")
        total_results['modules']['ACCOUNTS'] = {'total': 0, 'passed': 0, 'failed': 1, 'details': [f"âŒ Error crÃ­tico: {str(e)}"]}
        total_results['total_failed'] += 1
    
    # 3. TRANSACTIONS
    try:
        transactions_results = test_transactions_module()
        total_results['modules']['TRANSACTIONS'] = transactions_results
        total_results['total_tests'] += transactions_results['total']
        total_results['total_passed'] += transactions_results['passed']
        total_results['total_failed'] += transactions_results['failed']
    except Exception as e:
        print_error(f"Error crÃ­tico en mÃ³dulo TRANSACTIONS: {str(e)}")
        total_results['modules']['TRANSACTIONS'] = {'total': 0, 'passed': 0, 'failed': 1, 'details': [f"âŒ Error crÃ­tico: {str(e)}"]}
        total_results['total_failed'] += 1
    
    # 4. ANALYTICS
    try:
        analytics_results = test_analytics_module()
        total_results['modules']['ANALYTICS'] = analytics_results
        total_results['total_tests'] += analytics_results['total']
        total_results['total_passed'] += analytics_results['passed']
        total_results['total_failed'] += analytics_results['failed']
    except Exception as e:
        print_error(f"Error crÃ­tico en mÃ³dulo ANALYTICS: {str(e)}")
        total_results['modules']['ANALYTICS'] = {'total': 0, 'passed': 0, 'failed': 1, 'details': [f"âŒ Error crÃ­tico: {str(e)}"]}
        total_results['total_failed'] += 1
    
    # 5. GOALS
    try:
        goals_results = test_goals_module()
        total_results['modules']['GOALS'] = goals_results
        total_results['total_tests'] += goals_results['total']
        total_results['total_passed'] += goals_results['passed']
        total_results['total_failed'] += goals_results['failed']
    except Exception as e:
        print_error(f"Error crÃ­tico en mÃ³dulo GOALS: {str(e)}")
        total_results['modules']['GOALS'] = {'total': 0, 'passed': 0, 'failed': 1, 'details': [f"âŒ Error crÃ­tico: {str(e)}"]}
        total_results['total_failed'] += 1
    
    total_results['end_time'] = datetime.now()
    
    # === GENERAR REPORTE FINAL ===
    generate_final_report(total_results)
    
    return total_results

def generate_final_report(results):
    """Genera reporte final completo"""
    print_section("REPORTE FINAL DE TESTING")
    
    # Calcular duraciÃ³n
    duration = results['end_time'] - results['start_time']
    
    # Resumen general
    print(f"{Colors.BOLD}ðŸ“Š RESUMEN GENERAL:{Colors.END}")
    print(f"  Total de tests ejecutados: {Colors.BOLD}{results['total_tests']}{Colors.END}")
    print(f"  Tests exitosos: {Colors.GREEN}{results['total_passed']}{Colors.END}")
    print(f"  Tests fallidos: {Colors.RED}{results['total_failed']}{Colors.END}")
    print(f"  DuraciÃ³n total: {Colors.CYAN}{duration.total_seconds():.2f} segundos{Colors.END}")
    
    # Calcular porcentaje de Ã©xito
    if results['total_tests'] > 0:
        success_rate = (results['total_passed'] / results['total_tests']) * 100
        print(f"  Tasa de Ã©xito: {Colors.BOLD}{success_rate:.1f}%{Colors.END}")
    
    print()
    
    # Resultados por mÃ³dulo
    print(f"{Colors.BOLD}ðŸ“‹ RESULTADOS POR MÃ“DULO:{Colors.END}")
    
    for module_name, module_results in results['modules'].items():
        total = module_results['total']
        passed = module_results['passed']
        failed = module_results['failed']
        
        if total > 0:
            success_rate = (passed / total) * 100
            status_color = Colors.GREEN if success_rate >= 80 else Colors.YELLOW if success_rate >= 60 else Colors.RED
        else:
            success_rate = 0
            status_color = Colors.RED
        
        print(f"\n  {Colors.BOLD}{module_name}:{Colors.END}")
        print(f"    Tests: {total} | Exitosos: {status_color}{passed}{Colors.END} | Fallidos: {Colors.RED}{failed}{Colors.END} | Ã‰xito: {status_color}{success_rate:.1f}%{Colors.END}")
        
        # Mostrar detalles
        for detail in module_results['details']:
            print(f"    {detail}")
    
    # Status final
    print(f"\n{Colors.BOLD}ðŸ STATUS FINAL:{Colors.END}")
    
    if results['total_failed'] == 0:
        print(f"{Colors.GREEN}{Colors.BOLD}ðŸŽ‰ TODOS LOS TESTS PASARON EXITOSAMENTE!{Colors.END}")
        print(f"{Colors.GREEN}El sistema FinTrack estÃ¡ funcionando correctamente.{Colors.END}")
    elif results['total_passed'] > results['total_failed']:
        print(f"{Colors.YELLOW}{Colors.BOLD}âš ï¸ ALGUNOS TESTS FALLARON{Colors.END}")
        print(f"{Colors.YELLOW}El sistema funciona parcialmente. Revisar errores.{Colors.END}")
    else:
        print(f"{Colors.RED}{Colors.BOLD}ðŸš¨ MÃšLTIPLES FALLAS DETECTADAS{Colors.END}")
        print(f"{Colors.RED}El sistema requiere atenciÃ³n inmediata.{Colors.END}")
    
    # InformaciÃ³n adicional
    print(f"\n{Colors.BLUE}ðŸ’¡ INFORMACIÃ“N ADICIONAL:{Colors.END}")
    
    if TEST_CONFIG.get('tokens'):
        print(f"  âœ… Token JWT obtenido y vÃ¡lido")
    else:
        print(f"  âŒ No se pudo obtener token JWT")
    
    accounts_count = len(TEST_CONFIG['created_resources']['accounts'])
    transactions_count = len(TEST_CONFIG['created_resources']['transactions'])
    goals_count = len(TEST_CONFIG['created_resources']['goals'])
    
    print(f"  ðŸ“Š Recursos creados durante testing:")
    print(f"    - Cuentas: {accounts_count}")
    print(f"    - Transacciones: {transactions_count}")
    print(f"    - Metas: {goals_count}")
    
    # Recomendaciones
    print(f"\n{Colors.CYAN}ðŸŽ¯ RECOMENDACIONES:{Colors.END}")
    
    if results['total_failed'] > 0:
        print(f"  â€¢ Revisar logs del servidor para errores especÃ­ficos")
        print(f"  â€¢ Verificar configuraciÃ³n de base de datos")
        print(f"  â€¢ Comprobar variables de entorno")
        print(f"  â€¢ Validar permisos de autenticaciÃ³n")
    
    print(f"  â€¢ Ejecutar migraciones si es necesario: python manage.py migrate")
    print(f"  â€¢ Verificar que el servidor estÃ© ejecutÃ¡ndose en: {BASE_URL}")
    
    print(f"\n{Colors.CYAN}{'='*80}{Colors.END}")
    print(f"{Colors.CYAN}ðŸ§ª TESTING COMPLETADO - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}{Colors.END}")
    print(f"{Colors.CYAN}{'='*80}{Colors.END}")

# =============================================================================
# ðŸš€ PUNTO DE ENTRADA
# =============================================================================

def generate_module_report(module_name, results):
    """Genera reporte para un mÃ³dulo especÃ­fico"""
    print_section(f"REPORTE DE MÃ“DULO {module_name}")
    
    total = results['total']
    passed = results['passed']
    failed = results['failed']
    
    print(f"{Colors.BOLD}ðŸ“Š RESUMEN DE {module_name}:{Colors.END}")
    print(f"  Total de tests: {total}")
    print(f"  Tests exitosos: {Colors.GREEN}{passed}{Colors.END}")
    print(f"  Tests fallidos: {Colors.RED}{failed}{Colors.END}")
    
    if total > 0:
        success_rate = (passed / total) * 100
        print(f"  Tasa de Ã©xito: {Colors.BOLD}{success_rate:.1f}%{Colors.END}")
    
    print(f"\n{Colors.BOLD}ðŸ“‹ DETALLES:{Colors.END}")
    for detail in results['details']:
        print(f"  {detail}")
    
    if failed == 0:
        print(f"\n{Colors.GREEN}âœ… MÃ³dulo {module_name} funcionando correctamente!{Colors.END}")
    else:
        print(f"\n{Colors.YELLOW}âš ï¸ MÃ³dulo {module_name} requiere atenciÃ³n.{Colors.END}")

def cleanup_test_resources():
    """Limpia recursos creados durante el testing"""
    if not TEST_CONFIG.get('tokens', {}).get('access'):
        print_warning("No hay token de autenticaciÃ³n - saltando cleanup")
        return
    
    print_info("ðŸ§¹ Iniciando limpieza de recursos de prueba...")
    cleanup_count = 0
    
    # Limpiar transacciones creadas
    for transaction_id in TEST_CONFIG['created_resources']['transactions']:
        try:
            response = make_request("DELETE", f"/transactions/{transaction_id}/")
            if response and response.status_code == 204:
                cleanup_count += 1
        except:
            pass  # Ignorar errores de cleanup
    
    # Limpiar metas creadas
    for goal_id in TEST_CONFIG['created_resources']['goals']:
        try:
            response = make_request("DELETE", f"/goals/{goal_id}/")
            if response and response.status_code == 204:
                cleanup_count += 1
        except:
            pass  # Ignorar errores de cleanup
    
    # Limpiar cuentas creadas (solo las que creamos nosotros)
    for account_id in TEST_CONFIG['created_resources']['accounts']:
        # Solo intentar eliminar si sabemos que la creamos nosotros
        # (las cuentas demo no se deben eliminar)
        try:
            account_response = make_request("GET", f"/accounts/{account_id}/")
            if account_response and account_response.status_code == 200:
                account_data = account_response.json()
                if account_data.get('name') == 'Cuenta Test':  # Solo nuestras cuentas de prueba
                    delete_response = make_request("DELETE", f"/accounts/{account_id}/")
                    if delete_response and delete_response.status_code == 204:
                        cleanup_count += 1
        except:
            pass  # Ignorar errores de cleanup
    
    if cleanup_count > 0:
        print_success(f"âœ… Limpieza completada - {cleanup_count} recursos eliminados")
    else:
        print_info("â„¹ï¸ No hay recursos para limpiar")

if __name__ == "__main__":
    print(f"{Colors.BOLD}ðŸš€ FinTrack Test Suite v1.0{Colors.END}")
    print(f"Desarrollado para pruebas automÃ¡ticas del sistema FinTrack")
    print()
    
    # Verificar argumentos de lÃ­nea de comandos
    if len(sys.argv) > 1:
        if sys.argv[1] == "--help" or sys.argv[1] == "-h":
            print("Uso: python test_all_endpoints.py [opciones]")
            print("Opciones:")
            print("  --help, -h       Mostrar esta ayuda")
            print("  --url URL        Especificar URL base personalizada")
            print("  --timeout N      Timeout en segundos (default: 30)")
            print("  --no-cleanup     No limpiar recursos creados")
            print("  --module MODULE  Ejecutar solo un mÃ³dulo especÃ­fico")
            print("  --verbose        Mostrar informaciÃ³n detallada")
            print()
            print("MÃ³dulos disponibles: core, accounts, transactions, analytics, goals")
            print()
            print("Ejemplos:")
            print("  python test_all_endpoints.py")
            print("  python test_all_endpoints.py --url http://192.168.1.100:8000/api")
            print("  python test_all_endpoints.py --module core")
            print("  python test_all_endpoints.py --timeout 60 --verbose")
            sys.exit(0)
        
        elif sys.argv[1] == "--url" and len(sys.argv) > 2:
            BASE_URL = sys.argv[2]
            print(f"ðŸŒ URL personalizada: {BASE_URL}")
        
        elif sys.argv[1] == "--timeout" and len(sys.argv) > 2:
            TEST_CONFIG['timeout'] = int(sys.argv[2])
            print(f"â° Timeout configurado: {TEST_CONFIG['timeout']} segundos")
        
        elif sys.argv[1] == "--module" and len(sys.argv) > 2:
            module_name = sys.argv[2].lower()
            
            if module_name == "core":
                print(f"ðŸŽ¯ Ejecutando solo mÃ³dulo CORE")
                core_results = test_core_module()
                generate_module_report("CORE", core_results)
            elif module_name == "accounts":
                print(f"ðŸŽ¯ Ejecutando solo mÃ³dulo ACCOUNTS")
                accounts_results = test_accounts_module()
                generate_module_report("ACCOUNTS", accounts_results)
            elif module_name == "transactions":
                print(f"ðŸŽ¯ Ejecutando solo mÃ³dulo TRANSACTIONS")
                transactions_results = test_transactions_module()
                generate_module_report("TRANSACTIONS", transactions_results)
            elif module_name == "analytics":
                print(f"ðŸŽ¯ Ejecutando solo mÃ³dulo ANALYTICS")
                analytics_results = test_analytics_module()
                generate_module_report("ANALYTICS", analytics_results)
            elif module_name == "goals":
                print(f"ðŸŽ¯ Ejecutando solo mÃ³dulo GOALS")
                goals_results = test_goals_module()
                generate_module_report("GOALS", goals_results)
            else:
                print_error(f"MÃ³dulo desconocido: {module_name}")
                print("MÃ³dulos disponibles: core, accounts, transactions, analytics, goals")
                sys.exit(1)
            sys.exit(0)
        
        elif sys.argv[1] == "--verbose":
            print("ðŸ” Modo verbose activado")
            # Se puede implementar mÃ¡s logging detallado
        
        elif sys.argv[1] == "--no-cleanup":
            print("ðŸš« Modo no-cleanup - Los recursos creados no se eliminarÃ¡n")
            TEST_CONFIG['cleanup'] = False
    
    try:
        # Verificar conectividad inicial
        print_info(f"Verificando conectividad con {BASE_URL}...")
        
        # Test bÃ¡sico de conectividad (sin autenticaciÃ³n)
        test_response = make_request("GET", "/", auth_required=False)
        if test_response is None:
            print_error("âŒ No se puede conectar al servidor")
            print_error("Verificar que:")
            print_error("  â€¢ El servidor Django estÃ© ejecutÃ¡ndose")
            print_error("  â€¢ La URL base sea correcta")
            print_error("  â€¢ No haya problemas de red/firewall")
            sys.exit(1)
        
        print_success(f"âœ… Conectividad establecida")
        
        # Ejecutar todos los tests
        final_results = run_all_tests()
        
        # Cleanup opcional de recursos creados
        if TEST_CONFIG.get('cleanup', True):
            cleanup_test_resources()
        
        # Exit code basado en resultados
        if final_results['total_failed'] == 0:
            print(f"\n{Colors.GREEN}ðŸŽ‰ Todos los tests completados exitosamente!{Colors.END}")
            sys.exit(0)
        elif final_results['total_passed'] > final_results['total_failed']:
            print(f"\n{Colors.YELLOW}âš ï¸ Tests completados con algunas fallas.{Colors.END}")
            sys.exit(1)
        else:
            print(f"\n{Colors.RED}âŒ Tests completados con mÃºltiples fallas crÃ­ticas.{Colors.END}")
            sys.exit(2)
            
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}âš ï¸ Testing interrumpido por el usuario{Colors.END}")
        cleanup_test_resources()
        sys.exit(130)
        
    except Exception as e:
        print_error(f"Error crÃ­tico durante el testing: {str(e)}")
        cleanup_test_resources()
        sys.exit(1)

# =============================================================================
# ðŸ”§ FUNCIONES AUXILIARES ADICIONALES
# =============================================================================

def test_endpoint_with_pagination(endpoint, description=""):
    """Prueba endpoint con soporte de paginaciÃ³n"""
    print_test(endpoint, "GET", f"{description} (con paginaciÃ³n)")
    
    # Test pÃ¡gina 1
    response = make_request("GET", endpoint, params={"page": 1, "page_size": 5})
    data = validate_response(response, 200, endpoint)
    
    if not data:
        return False
    
    # Verificar estructura de paginaciÃ³n
    if isinstance(data, dict) and 'results' in data:
        print_info(f"PaginaciÃ³n detectada - Total: {data.get('count', 'N/A')}")
        return True
    elif isinstance(data, list):
        print_info(f"Lista simple - {len(data)} elementos")
        return True
    
    return False

def test_endpoint_filters(endpoint, filters, description=""):
    """Prueba mÃºltiples filtros en un endpoint"""
    print_test(endpoint, "GET", f"{description} (con filtros)")
    
    results = []
    
    for filter_name, filter_value in filters.items():
        try:
            response = make_request("GET", endpoint, params={filter_name: filter_value})
            if response and response.status_code == 200:
                results.append(f"âœ… Filtro {filter_name}={filter_value}")
            else:
                results.append(f"âŒ Filtro {filter_name}={filter_value}")
        except:
            results.append(f"âŒ Error filtro {filter_name}={filter_value}")
    
    return results

def validate_jwt_token():
    """Valida que el token JWT estÃ© funcionando correctamente"""
    if not TEST_CONFIG.get('tokens', {}).get('access'):
        return False
    
    # Probar endpoint que requiere autenticaciÃ³n
    response = make_request("GET", "/auth/profile/")
    return response is not None and response.status_code == 200

def test_crud_operations(endpoint, create_data, update_data, entity_name="recurso"):
    """Prueba operaciones CRUD completas en un endpoint"""
    results = {
        'create': False,
        'read': False,
        'update': False,
        'delete': False,
        'created_id': None
    }
    
    # CREATE
    print_test(endpoint, "POST", f"Crear {entity_name}")
    response = make_request("POST", endpoint, data=create_data)
    create_result = validate_response(response, 201, endpoint)
    
    if create_result and create_result.get('id'):
        results['create'] = True
        results['created_id'] = create_result['id']
        print_success(f"âœ… {entity_name} creado - ID: {results['created_id']}")
        
        # READ
        print_test(f"{endpoint}{results['created_id']}/", "GET", f"Leer {entity_name}")
        response = make_request("GET", f"{endpoint}{results['created_id']}/")
        read_result = validate_response(response, 200, f"{endpoint}{results['created_id']}/")
        
        if read_result:
            results['read'] = True
            print_success(f"âœ… {entity_name} leÃ­do correctamente")
            
            # UPDATE
            print_test(f"{endpoint}{results['created_id']}/", "PATCH", f"Actualizar {entity_name}")
            response = make_request("PATCH", f"{endpoint}{results['created_id']}/", data=update_data)
            update_result = validate_response(response, 200, f"{endpoint}{results['created_id']}/")
            
            if update_result:
                results['update'] = True
                print_success(f"âœ… {entity_name} actualizado correctamente")
            
            # DELETE
            print_test(f"{endpoint}{results['created_id']}/", "DELETE", f"Eliminar {entity_name}")
            response = make_request("DELETE", f"{endpoint}{results['created_id']}/")
            
            if response and response.status_code == 204:
                results['delete'] = True
                print_success(f"âœ… {entity_name} eliminado correctamente")
            else:
                print_error(f"âŒ Error eliminando {entity_name}")
        else:
            print_error(f"âŒ Error leyendo {entity_name}")
    else:
        print_error(f"âŒ Error creando {entity_name}")
    
    return results

def performance_test(endpoint, iterations=5, description=""):
    """Realiza test bÃ¡sico de rendimiento en un endpoint"""
    print_test(endpoint, "GET", f"Test de rendimiento ({iterations} iteraciones)")
    
    times = []
    successful_requests = 0
    
    for i in range(iterations):
        start_time = time.time()
        response = make_request("GET", endpoint)
        end_time = time.time()
        
        request_time = end_time - start_time
        times.append(request_time)
        
        if response and response.status_code == 200:
            successful_requests += 1
        
        time.sleep(0.1)  # PequeÃ±a pausa entre requests
    
    if times:
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        print_info(f"ðŸ“ˆ Rendimiento: {successful_requests}/{iterations} exitosos")
        print_info(f"â±ï¸ Tiempo promedio: {avg_time:.3f}s (min: {min_time:.3f}s, max: {max_time:.3f}s)")
        
        return {
            'success_rate': successful_requests / iterations,
            'avg_time': avg_time,
            'min_time': min_time,
            'max_time': max_time
        }
    
    return None

def export_test_report(results, filename=None):
    """Exporta el reporte de testing a un archivo JSON"""
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"fintrack_test_report_{timestamp}.json"
    
    try:
        # Preparar datos para JSON (convertir datetime a string)
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'base_url': BASE_URL,
            'total_tests': results['total_tests'],
            'total_passed': results['total_passed'],
            'total_failed': results['total_failed'],
            'success_rate': (results['total_passed'] / results['total_tests'] * 100) if results['total_tests'] > 0 else 0,
            'duration_seconds': (results['end_time'] - results['start_time']).total_seconds(),
            'modules': results['modules'],
            'test_config': {
                'timeout': TEST_CONFIG['timeout'],
                'max_retries': TEST_CONFIG['max_retries']
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print_success(f"ðŸ“„ Reporte exportado a: {filename}")
        return filename
        
    except Exception as e:
        print_error(f"Error exportando reporte: {str(e)}")
        return None

# =============================================================================
# ðŸŽ¯ TESTS ESPECIALIZADOS ADICIONALES
# =============================================================================

def test_authentication_flow():
    """Prueba completa del flujo de autenticaciÃ³n"""
    print_section("PRUEBA DE FLUJO DE AUTENTICACIÃ“N COMPLETO")
    
    results = {
        'demo_user_creation': False,
        'token_obtain': False,
        'token_usage': False,
        'token_refresh': False,
        'profile_access': False
    }
    
    # 1. Crear usuario demo
    response = make_request("POST", "/auth/demo/", auth_required=False)
    if response and response.status_code == 200:
        data = response.json()
        if data.get('access') and data.get('refresh'):
            results['demo_user_creation'] = True
            results['token_obtain'] = True
            
            # Guardar tokens temporalmente
            temp_tokens = {
                'access': data['access'],
                'refresh': data['refresh']
            }
            
            # 2. Usar token para acceder a endpoint protegido
            headers = {'Authorization': f"Bearer {temp_tokens['access']}"}
            profile_response = make_request("GET", "/auth/profile/", headers=headers, auth_required=False)
            
            if profile_response and profile_response.status_code == 200:
                results['token_usage'] = True
                results['profile_access'] = True
            
            # 3. Refrescar token
            refresh_data = {"refresh": temp_tokens['refresh']}
            refresh_response = make_request("POST", "/token/refresh/", data=refresh_data, auth_required=False)
            
            if refresh_response and refresh_response.status_code == 200:
                refresh_result = refresh_response.json()
                if refresh_result.get('access'):
                    results['token_refresh'] = True
    
    # Mostrar resultados
    print(f"\nðŸ“Š RESULTADOS DEL FLUJO DE AUTENTICACIÃ“N:")
    for step, success in results.items():
        status = "âœ…" if success else "âŒ"
        print(f"  {status} {step.replace('_', ' ').title()}")
    
    success_count = sum(results.values())
    total_steps = len(results)
    
    print(f"\nðŸŽ¯ Flujo de autenticaciÃ³n: {success_count}/{total_steps} pasos exitosos")
    
    return results

def test_data_consistency():
    """Verifica consistencia de datos entre mÃ³dulos relacionados"""
    print_section("PRUEBA DE CONSISTENCIA DE DATOS")
    
    if not TEST_CONFIG.get('tokens', {}).get('access'):
        print_error("Sin autenticaciÃ³n - saltando pruebas de consistencia")
        return False
    
    consistency_checks = []
    
    # 1. Verificar que todas las transacciones tienen cuentas vÃ¡lidas
    print_test("/transactions/", "GET", "Verificando consistencia transacciones-cuentas")
    transactions_response = make_request("GET", "/transactions/")
    accounts_response = make_request("GET", "/accounts/")
    
    if transactions_response and accounts_response:
        transactions_data = transactions_response.json()
        accounts_data = accounts_response.json()
        
        # Extraer IDs de cuentas
        account_ids = set()
        accounts_list = accounts_data.get('results', accounts_data) if isinstance(accounts_data, dict) else accounts_data
        for account in accounts_list:
            account_ids.add(account['id'])
        
        # Verificar transacciones
        transactions_list = transactions_data.get('results', transactions_data) if isinstance(transactions_data, dict) else transactions_data
        invalid_transactions = []
        
        for transaction in transactions_list:
            if transaction.get('from_account') and transaction['from_account'] not in account_ids:
                invalid_transactions.append(f"TransacciÃ³n {transaction['id']} - cuenta origen invÃ¡lida")
            if transaction.get('to_account') and transaction['to_account'] not in account_ids:
                invalid_transactions.append(f"TransacciÃ³n {transaction['id']} - cuenta destino invÃ¡lida")
        
        if not invalid_transactions:
            consistency_checks.append("âœ… Consistencia transacciones-cuentas: OK")
        else:
            consistency_checks.append(f"âŒ Inconsistencias encontradas: {len(invalid_transactions)}")
    
    # 2. Verificar que todas las metas tienen cuentas asociadas vÃ¡lidas
    print_test("/goals/", "GET", "Verificando consistencia metas-cuentas")
    goals_response = make_request("GET", "/goals/")
    
    if goals_response and accounts_response:
        goals_data = goals_response.json()
        goals_list = goals_data.get('results', goals_data) if isinstance(goals_data, dict) else goals_data
        
        invalid_goals = []
        for goal in goals_list:
            if goal.get('associated_account') and goal['associated_account'] not in account_ids:
                invalid_goals.append(f"Meta {goal['id']} - cuenta asociada invÃ¡lida")
        
        if not invalid_goals:
            consistency_checks.append("âœ… Consistencia metas-cuentas: OK")
        else:
            consistency_checks.append(f"âŒ Inconsistencias en metas: {len(invalid_goals)}")
    
    # Mostrar resultados de consistencia
    print(f"\nðŸ“Š RESULTADOS DE CONSISTENCIA:")
    for check in consistency_checks:
        print(f"  {check}")
    
    return len([c for c in consistency_checks if c.startswith("âœ…")]) == len(consistency_checks)

# Fin de la continuaciÃ³n del archivo
print("\n" + "="*80)
print("ðŸ§ª FinTrack Test Suite - Sistema de Testing Completo")
print("Desarrollado para validaciÃ³n automÃ¡tica de todos los endpoints")
print("="*80)